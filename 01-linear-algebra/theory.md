# Linear Algebra for Machine Learning - Theory

**The Mathematical Foundation of Modern AI**

---

## Table of Contents

1. [Why Linear Algebra?](#1-why-linear-algebra)
2. [Scalars, Vectors, and Matrices](#2-scalars-vectors-and-matrices)
3. [Vector Operations](#3-vector-operations)
4. [Matrix Operations](#4-matrix-operations)
5. [Matrix Multiplication Deep Dive](#5-matrix-multiplication-deep-dive)
6. [Linear Transformations](#6-linear-transformations)
7. [Systems of Linear Equations](#7-systems-of-linear-equations)
8. [Matrix Rank and Span](#8-matrix-rank-and-span)
9. [Determinants](#9-determinants)
10. [Eigenvalues and Eigenvectors](#10-eigenvalues-and-eigenvectors)
11. [Matrix Decompositions](#11-matrix-decompositions)
12. [Norms and Distances](#12-norms-and-distances)
13. [Orthogonality and Projections](#13-orthogonality-and-projections)
14. [Computational Considerations](#14-computational-considerations)
15. [ML Applications](#15-ml-applications)

---

## 1. Why Linear Algebra?

### The Language of Machine Learning

**Every ML algorithm is built on linear algebra:**

- **Neural Networks**: Matrix multiplications and transformations
- **PCA**: Eigenvalue decomposition
- **SVD**: Collaborative filtering, dimensionality reduction
- **Linear Regression**: Matrix inversion, projections
- **Gradient Descent**: Vector operations
- **Covariance**: Matrix operations
- **Transformers**: Attention is matrix operations

### The Power of Linear Algebra

**Why is linear algebra so powerful for ML?**

1. **Compact Representation**: Express operations on millions of numbers in one line
2. **Geometric Intuition**: Understand transformations visually
3. **Computational Efficiency**: Optimized matrix operations (GPU acceleration)
4. **Generalization**: Same concepts work in any dimension
5. **Theoretical Foundation**: Rigorous mathematical framework

### ML Without Linear Algebra?

**Impossible.** You'd be:
- Writing millions of individual operations
- Missing geometric intuition
- Unable to use GPUs effectively
- Reinventing the wheel constantly

**With Linear Algebra:**
- Express complex operations concisely
- Understand what operations do geometrically
- Leverage optimized libraries (BLAS, cuBLAS)
- Build on established theory

---

## 2. Scalars, Vectors, and Matrices

### Scalars

A **scalar** is a single number.

**Examples:**
```
x = 5
Œ± = 0.001 (learning rate)
Œª = 2.5 (regularization parameter)
```

**In ML:**
- Loss values
- Learning rates
- Hyperparameters
- Predictions (regression)

### Vectors

A **vector** is an ordered array of numbers.

**Notation:**
```
      ‚é° 1 ‚é§
  v = ‚é¢ 2 ‚é•  (column vector, preferred in ML)
      ‚é£ 3 ‚é¶

  v = [1, 2, 3]  (row vector)
```

**Dimensions:**
- Length n (number of elements)
- Lives in ‚Ñù‚Åø (n-dimensional real space)

**Geometric Interpretation:**
- Point in n-dimensional space
- Arrow from origin to that point
- Direction and magnitude

**In ML:**
```python
# A single data sample
x = [age, income, height]

# Model parameters
w = [w‚ÇÅ, w‚ÇÇ, w‚ÇÉ]  # weights

# Predictions for a batch
y_pred = [0.8, 0.3, 0.9, 0.1]
```

### Matrices

A **matrix** is a 2D array of numbers.

**Notation:**
```
      ‚é° 1  2  3 ‚é§
  A = ‚é¢ 4  5  6 ‚é•  (2√ó3 matrix)
      ‚é£ 7  8  9 ‚é¶
```

**Dimensions:**
- m √ó n matrix (m rows, n columns)
- Lives in ‚Ñù·µêÀ£‚Åø

**Indexing:**
- A·µ¢‚±º = element at row i, column j
- Indices typically start at 1 (math) or 0 (programming)

**In ML:**
```python
# Dataset: 1000 samples, 5 features
X = [[x‚ÇÅ‚ÇÅ, x‚ÇÅ‚ÇÇ, x‚ÇÅ‚ÇÉ, x‚ÇÅ‚ÇÑ, x‚ÇÅ‚ÇÖ],
     [x‚ÇÇ‚ÇÅ, x‚ÇÇ‚ÇÇ, x‚ÇÇ‚ÇÉ, x‚ÇÇ‚ÇÑ, x‚ÇÇ‚ÇÖ],
     ...
     [x‚ÇÅ‚ÇÄ‚ÇÄ‚ÇÄ,‚ÇÅ, ..., x‚ÇÅ‚ÇÄ‚ÇÄ‚ÇÄ,‚ÇÖ]]

# Weight matrix in neural network
W = [[w‚ÇÅ‚ÇÅ, w‚ÇÅ‚ÇÇ, w‚ÇÅ‚ÇÉ],
     [w‚ÇÇ‚ÇÅ, w‚ÇÇ‚ÇÇ, w‚ÇÇ‚ÇÉ]]
```

### Tensors

A **tensor** is a generalization to higher dimensions.

**Dimensions:**
- 0D tensor: scalar
- 1D tensor: vector
- 2D tensor: matrix
- 3D tensor: cube of numbers
- nD tensor: n-dimensional array

**In ML:**
```python
# Batch of images: (batch_size, height, width, channels)
images = np.zeros((32, 224, 224, 3))  # 4D tensor

# Video: (batch, time, height, width, channels)
video = np.zeros((16, 30, 224, 224, 3))  # 5D tensor
```

---

## 3. Vector Operations

### Vector Addition

**Element-wise addition:**

```
‚é° 1 ‚é§   ‚é° 4 ‚é§   ‚é° 5 ‚é§
‚é¢ 2 ‚é• + ‚é¢ 5 ‚é• = ‚é¢ 7 ‚é•
‚é£ 3 ‚é¶   ‚é£ 6 ‚é¶   ‚é£ 9 ‚é¶
```

**Properties:**
- Commutative: a + b = b + a
- Associative: (a + b) + c = a + (b + c)
- Identity: a + 0 = a

**Geometric Interpretation:**
- Parallelogram rule
- "Walk" along first vector, then second

**In ML:**
```python
# Adding bias in neural networks
output = Wx + b  # b is added to each element
```

### Scalar Multiplication

**Multiply each element by scalar:**

```
    ‚é° 1 ‚é§   ‚é° 2 ‚é§
2 √ó ‚é¢ 2 ‚é• = ‚é¢ 4 ‚é•
    ‚é£ 3 ‚é¶   ‚é£ 6 ‚é¶
```

**Properties:**
- Distributive: c(a + b) = ca + cb
- Associative: c‚ÇÅ(c‚ÇÇa) = (c‚ÇÅc‚ÇÇ)a

**Geometric Interpretation:**
- Scales the vector's magnitude
- Negative scalar flips direction

**In ML:**
```python
# Learning rate scaling gradient
update = -learning_rate * gradient
```

### Dot Product (Inner Product)

**Definition:**
```
a ¬∑ b = a‚ÇÅb‚ÇÅ + a‚ÇÇb‚ÇÇ + ... + a‚Çôb‚Çô = Œ£ a·µ¢b·µ¢
```

**Example:**
```
[1, 2, 3] ¬∑ [4, 5, 6] = 1√ó4 + 2√ó5 + 3√ó6 = 4 + 10 + 18 = 32
```

**Properties:**
- Commutative: a ¬∑ b = b ¬∑ a
- Distributive: a ¬∑ (b + c) = a ¬∑ b + a ¬∑ c
- Scalar multiplication: (ca) ¬∑ b = c(a ¬∑ b)

**Geometric Interpretation:**
```
a ¬∑ b = ||a|| √ó ||b|| √ó cos(Œ∏)
```
Where Œ∏ is the angle between vectors.

**Important Cases:**
- Œ∏ = 0¬∞: a ¬∑ b = ||a|| √ó ||b|| (parallel, same direction)
- Œ∏ = 90¬∞: a ¬∑ b = 0 (perpendicular, orthogonal)
- Œ∏ = 180¬∞: a ¬∑ b = -||a|| √ó ||b|| (opposite direction)

**In ML:**
```python
# Linear regression prediction
y_pred = w ¬∑ x + b

# Similarity measure
similarity = vec1 ¬∑ vec2

# Attention mechanism
attention_score = query ¬∑ key
```

### Vector Norm (Length)

**L2 Norm (Euclidean length):**
```
||v||‚ÇÇ = ‚àö(v‚ÇÅ¬≤ + v‚ÇÇ¬≤ + ... + v‚Çô¬≤) = ‚àö(v ¬∑ v)
```

**L1 Norm (Manhattan distance):**
```
||v||‚ÇÅ = |v‚ÇÅ| + |v‚ÇÇ| + ... + |v‚Çô|
```

**L‚àû Norm (Maximum norm):**
```
||v||‚àû = max(|v‚ÇÅ|, |v‚ÇÇ|, ..., |v‚Çô|)
```

**Properties:**
- ||v|| ‚â• 0
- ||v|| = 0 ‚ü∫ v = 0
- ||cv|| = |c| √ó ||v||
- Triangle inequality: ||a + b|| ‚â§ ||a|| + ||b||

**In ML:**
```python
# Regularization
L2_penalty = Œª √ó ||w||‚ÇÇ¬≤
L1_penalty = Œª √ó ||w||‚ÇÅ

# Gradient clipping
if ||gradient|| > threshold:
    gradient = gradient √ó threshold / ||gradient||
```

### Unit Vector (Normalization)

**Definition:**
```
√ª = v / ||v||
```

**Properties:**
- ||√ª|| = 1
- Points in same direction as v
- Essential for many algorithms

**In ML:**
```python
# Normalize embeddings
normalized_embedding = embedding / ||embedding||

# Cosine similarity works on normalized vectors
cosine_sim = normalized_a ¬∑ normalized_b
```

### Cross Product (3D only)

**Definition (for 3D vectors):**
```
a √ó b = ‚é° a‚ÇÇb‚ÇÉ - a‚ÇÉb‚ÇÇ ‚é§
        ‚é¢ a‚ÇÉb‚ÇÅ - a‚ÇÅb‚ÇÉ ‚é•
        ‚é£ a‚ÇÅb‚ÇÇ - a‚ÇÇb‚ÇÅ ‚é¶
```

**Properties:**
- Result is perpendicular to both a and b
- ||a √ó b|| = ||a|| √ó ||b|| √ó sin(Œ∏)
- Anti-commutative: a √ó b = -(b √ó a)

**Less common in ML, but used in:**
- 3D computer vision
- Robotics
- Graphics

---

## 4. Matrix Operations

### Matrix Addition

**Element-wise:**
```
‚é° 1  2 ‚é§   ‚é° 5  6 ‚é§   ‚é° 6  8  ‚é§
‚é¢ 3  4 ‚é• + ‚é¢ 7  8 ‚é• = ‚é¢ 10  12 ‚é•
```

**Requirements:**
- Same dimensions (m √ó n)

**Properties:**
- Same as vector addition

**In ML:**
```python
# Combining gradients
total_gradient = gradient1 + gradient2

# Adding regularization
loss_with_reg = loss + regularization_term
```

### Scalar Multiplication

**Multiply each element:**
```
    ‚é° 1  2 ‚é§   ‚é° 2  4 ‚é§
2 √ó ‚é¢ 3  4 ‚é• = ‚é¢ 6  8 ‚é•
```

**In ML:**
```python
# Scale learning rate
update = -0.01 * gradient
```

### Transpose

**Flip rows and columns:**
```
    ‚é° 1  2  3 ‚é§·µÄ   ‚é° 1  4 ‚é§
A = ‚é¢ 4  5  6 ‚é•  = ‚é¢ 2  5 ‚é•
                    ‚é£ 3  6 ‚é¶
```

**Notation:** A·µÄ

**Properties:**
- (A·µÄ)·µÄ = A
- (A + B)·µÄ = A·µÄ + B·µÄ
- (AB)·µÄ = B·µÄA·µÄ (order reverses!)
- (cA)·µÄ = cA·µÄ

**Dimensions:**
- If A is m √ó n, then A·µÄ is n √ó m

**In ML:**
```python
# Convert between row and column vectors
y = x.T

# Compute covariance matrix
Œ£ = (X - Œº)·µÄ(X - Œº) / n

# Backpropagation
grad_W = grad_output.T @ activations
```

### Hadamard Product (Element-wise)

**Element-wise multiplication:**
```
‚é° 1  2 ‚é§     ‚é° 5  6 ‚é§   ‚é° 5   12 ‚é§
‚é¢ 3  4 ‚é•  ‚äô  ‚é¢ 7  8 ‚é• = ‚é¢ 21  32 ‚é•
```

**Notation:** A ‚äô B or A * B (NumPy uses *)

**Requirements:**
- Same dimensions

**In ML:**
```python
# Applying activation functions element-wise
activated = sigmoid(z) ‚äô (1 - sigmoid(z))  # sigmoid derivative

# Attention mechanism
attended = values ‚äô attention_weights
```

---

## 5. Matrix Multiplication Deep Dive

### The Most Important Operation in ML

**Matrix multiplication is everywhere:**
- Forward pass in neural networks
- Backpropagation
- Transformers (attention)
- Convolutions (can be expressed as matrix multiply)

### Definition

**Matrix-vector multiplication:**
```
‚é° 1  2  3 ‚é§   ‚é° 1 ‚é§   ‚é° 1√ó1 + 2√ó2 + 3√ó3 ‚é§   ‚é° 14 ‚é§
‚é¢ 4  5  6 ‚é• √ó ‚é¢ 2 ‚é• = ‚é¢ 4√ó1 + 5√ó2 + 6√ó3 ‚é• = ‚é¢ 32 ‚é•
                ‚é£ 3 ‚é¶
```

**Matrix-matrix multiplication:**
```
      ‚é° 1  2 ‚é§
A =   ‚é¢ 3  4 ‚é•  (2√ó2)
      
      ‚é° 5  6 ‚é§
B =   ‚é¢ 7  8 ‚é•  (2√ó2)

      ‚é° 1√ó5+2√ó7  1√ó6+2√ó8 ‚é§   ‚é° 19  22 ‚é§
AB =  ‚é¢ 3√ó5+4√ó7  3√ó6+4√ó8 ‚é• = ‚é¢ 43  50 ‚é•
```

**General Rule:**
```
(AB)·µ¢‚±º = Œ£‚Çñ A·µ¢‚ÇñB‚Çñ‚±º
```

The element at row i, column j of AB is the dot product of:
- Row i of A
- Column j of B

### Dimension Requirements

**For A (m √ó n) and B (p √ó q):**
- Can multiply AB only if **n = p** (inner dimensions match)
- Result AB is **m √ó q** (outer dimensions)

**Memory tip:**
```
(m √ó n) √ó (n √ó q) = (m √ó q)
     ‚Üë       ‚Üë
  must match!
```

### Properties

**NOT Commutative:**
- AB ‚â† BA (in general)
- Order matters!

**Associative:**
- (AB)C = A(BC)

**Distributive:**
- A(B + C) = AB + AC
- (A + B)C = AC + BC

**Identity:**
- AI = IA = A (where I is identity matrix)

### Geometric Interpretation

**Matrix multiplication is function composition:**

```
y = Bx  (first apply B)
z = Ay  (then apply A)

Combined: z = A(Bx) = (AB)x
```

**Each matrix is a transformation:**
- Rotation
- Scaling
- Shearing
- Reflection
- Projection

### Why Order Matters

**Example:**
```
Rotate 90¬∞, then scale 2x ‚â† Scale 2x, then rotate 90¬∞
```

The order of transformations affects the final result!

### Computational Complexity

**Naive algorithm:**
- O(mnq) for (m√ón) √ó (n√óq)
- For square matrices (n√ón): O(n¬≥)

**Optimized algorithms:**
- Strassen's: O(n^2.807)
- Best theoretical: O(n^2.373)
- Practical: Use BLAS libraries (highly optimized)

**In ML:**
```
# Forward pass in neural network layer
# X: (batch_size, in_features)
# W: (in_features, out_features)
output = X @ W  # (batch_size, out_features)

# Complexity: O(batch_size √ó in_features √ó out_features)
# For batch=32, in=1000, out=500:
# 32 √ó 1000 √ó 500 = 16 million operations
```

### Block Matrix Multiplication

**Large matrices can be split into blocks:**

```
‚é° A‚ÇÅ‚ÇÅ  A‚ÇÅ‚ÇÇ ‚é§   ‚é° B‚ÇÅ‚ÇÅ  B‚ÇÅ‚ÇÇ ‚é§   ‚é° A‚ÇÅ‚ÇÅB‚ÇÅ‚ÇÅ+A‚ÇÅ‚ÇÇB‚ÇÇ‚ÇÅ  A‚ÇÅ‚ÇÅB‚ÇÅ‚ÇÇ+A‚ÇÅ‚ÇÇB‚ÇÇ‚ÇÇ ‚é§
‚é¢ A‚ÇÇ‚ÇÅ  A‚ÇÇ‚ÇÇ ‚é• √ó ‚é¢ B‚ÇÇ‚ÇÅ  B‚ÇÇ‚ÇÇ ‚é• = ‚é¢ A‚ÇÇ‚ÇÅB‚ÇÅ‚ÇÅ+A‚ÇÇ‚ÇÇB‚ÇÇ‚ÇÅ  A‚ÇÇ‚ÇÅB‚ÇÅ‚ÇÇ+A‚ÇÇ‚ÇÇB‚ÇÇ‚ÇÇ ‚é•
```

**Used for:**
- Parallel computation
- GPU optimization
- Memory efficiency

---

## 6. Linear Transformations

### What is a Linear Transformation?

A function T: ‚Ñù‚Åø ‚Üí ‚Ñù·µê that preserves:

1. **Addition:** T(u + v) = T(u) + T(v)
2. **Scalar multiplication:** T(cu) = cT(u)

**Every linear transformation can be represented as matrix multiplication!**

```
T(x) = Ax
```

### Examples of Linear Transformations

**2D Rotation (by angle Œ∏):**
```
R(Œ∏) = ‚é° cos(Œ∏)  -sin(Œ∏) ‚é§
       ‚é¢ sin(Œ∏)   cos(Œ∏) ‚é•
```

**2D Scaling:**
```
S = ‚é° s‚Çì   0  ‚é§
    ‚é¢ 0   s·µß  ‚é•
```

**2D Reflection (across x-axis):**
```
F = ‚é° 1   0 ‚é§
    ‚é¢ 0  -1 ‚é•
```

**Projection onto x-axis:**
```
P = ‚é° 1  0 ‚é§
    ‚é¢ 0  0 ‚é•
```

### Understanding Transformations Geometrically

**Key insight:** Matrix columns show where basis vectors go!

```
A = ‚é° a  b ‚é§
    ‚é¢ c  d ‚é•

Transforms:
  ‚é° 1 ‚é§      ‚é° a ‚é§
  ‚é¢ 0 ‚é•  ‚Üí   ‚é¢ c ‚é•

  ‚é° 0 ‚é§      ‚é° b ‚é§
  ‚é¢ 1 ‚é•  ‚Üí   ‚é¢ d ‚é•
```

### In Machine Learning

**Every layer in a neural network is a linear transformation followed by nonlinearity:**

```python
# Linear transformation
z = Wx + b

# Nonlinear activation
a = œÉ(z)  # e.g., ReLU, sigmoid
```

**Why nonlinearity?**
- Without it, stacking layers is just matrix multiplication
- Multiple linear transformations = one linear transformation
- Need nonlinearity to learn complex functions!

---

## 7. Systems of Linear Equations

### The Fundamental Problem

**Solve for x:**
```
2x‚ÇÅ + 3x‚ÇÇ = 8
x‚ÇÅ - x‚ÇÇ = -1
```

**Matrix form:**
```
‚é° 2   3 ‚é§   ‚é° x‚ÇÅ ‚é§   ‚é° 8  ‚é§
‚é¢ 1  -1 ‚é• √ó ‚é¢ x‚ÇÇ ‚é• = ‚é¢ -1 ‚é•

Ax = b
```

### Three Possibilities

1. **Unique solution**: Exactly one x satisfies Ax = b
2. **No solution**: No x satisfies Ax = b (inconsistent)
3. **Infinite solutions**: Many x satisfy Ax = b

**Depends on:**
- Rank of A
- Relationship between A and b

### Solving Methods

**Method 1: Matrix Inversion (if A is square and invertible)**
```
Ax = b
A‚Åª¬πAx = A‚Åª¬πb
x = A‚Åª¬πb
```

**Caution:**
- Only works if A is invertible
- Numerically unstable
- O(n¬≥) complexity
- Don't actually compute A‚Åª¬π!

**Method 2: Gaussian Elimination**
- Convert to row echelon form
- Back substitution
- More stable than inversion
- Still O(n¬≥)

**Method 3: LU Decomposition**
- A = LU (lower √ó upper triangular)
- Solve Ly = b, then Ux = y
- Faster for multiple b vectors
- O(n¬≥) decomposition, O(n¬≤) per solve

**Method 4: Iterative Methods**
- Conjugate Gradient
- GMRES
- For large, sparse systems
- Can be faster than O(n¬≥)

### In Machine Learning

**Linear regression:**
```
minimize ||Ax - b||¬≤

Solution: x = (A·µÄA)‚Åª¬πA·µÄb  (normal equations)
```

**But we use gradient descent instead because:**
- More stable numerically
- Works for non-convex problems
- Scales better to large datasets
- Doesn't require matrix inversion

---

## 8. Matrix Rank and Span

### Rank

**Definition:** 
Maximum number of linearly independent row/column vectors.

**Intuition:**
- Dimensionality of the output space
- How much information the matrix contains

**Properties:**
- rank(A) ‚â§ min(m, n) for m √ó n matrix
- rank(A) = rank(A·µÄ)
- rank(AB) ‚â§ min(rank(A), rank(B))

**Full Rank:**
- rank(A) = min(m, n)
- Columns (or rows) are linearly independent
- Maximum information

**Rank Deficient:**
- rank(A) < min(m, n)
- Columns (or rows) are linearly dependent
- Lost information / redundancy

### Span

**Definition:**
Set of all possible linear combinations of vectors.

```
span{v‚ÇÅ, v‚ÇÇ, ..., v‚Çô} = {c‚ÇÅv‚ÇÅ + c‚ÇÇv‚ÇÇ + ... + c‚Çôv‚Çô | c‚ÇÅ,...,c‚Çô ‚àà ‚Ñù}
```

**Column Space (Range):**
- span of columns of A
- All possible outputs of Ax

**Row Space:**
- span of rows of A
- All possible inputs that give non-zero output

**Null Space (Kernel):**
- {x | Ax = 0}
- Inputs that map to zero

### Rank-Nullity Theorem

```
rank(A) + nullity(A) = n  (number of columns)
```

**Intuition:**
- Dimensions must be accounted for
- Either contribute to range or kernel

### In Machine Learning

**Feature redundancy:**
- Low rank X means redundant features
- PCA removes this redundancy

**Model capacity:**
- Weight matrix rank limits expressiveness
- Low-rank bottleneck in autoencoders

**Gradient flow:**
- Rank-deficient Jacobian ‚Üí vanishing gradients
- Information bottleneck

---

## 9. Determinants

### Definition

**For 2√ó2:**
```
det(‚é° a  b ‚é§) = ad - bc
    ‚é¢ c  d ‚é•
```

**For 3√ó3 (expansion by minors):**
```
det(A) = a‚ÇÅ‚ÇÅC‚ÇÅ‚ÇÅ + a‚ÇÅ‚ÇÇC‚ÇÅ‚ÇÇ + a‚ÇÅ‚ÇÉC‚ÇÅ‚ÇÉ
```

Where C·µ¢‚±º are cofactors.

**For n√ón:**
- Recursive definition via cofactors
- O(n!) complexity naively
- O(n¬≥) with LU decomposition

### Geometric Interpretation

**Determinant = signed volume of transformation**

**2D:**
- |det(A)| = area of parallelogram formed by column vectors

**3D:**
- |det(A)| = volume of parallelepiped

**Sign:**
- Positive: preserves orientation
- Negative: flips orientation
- Zero: collapses to lower dimension

### Properties

1. **det(I) = 1**
2. **det(AB) = det(A) √ó det(B)**
3. **det(A·µÄ) = det(A)**
4. **det(A‚Åª¬π) = 1/det(A)**
5. **det(cA) = c‚Åødet(A)** (for n√ón matrix)
6. **Swapping rows multiplies det by -1**
7. **Row of zeros ‚Üí det = 0**
8. **Linearly dependent rows ‚Üí det = 0**

### Invertibility

**A is invertible ‚ü∫ det(A) ‚â† 0**

**Why?**
- det(A) = 0 means transformation collapses dimensionality
- Cannot recover original space
- No unique inverse

### In Machine Learning

**Checking invertibility:**
```python
if np.linalg.det(A) close to 0:
    # Matrix is nearly singular
    # Use regularization or pseudoinverse
```

**Change of variables (probability):**
```
p_y(y) = p_x(x) / |det(J)|
```
Where J is Jacobian matrix.

**Gaussian distribution:**
```
N(Œº, Œ£) ‚àù exp(-¬Ω(x-Œº)·µÄŒ£‚Åª¬π(x-Œº)) / ‚àödet(Œ£)
```

**Numerical stability:**
- Computing det directly is unstable
- Use log-determinant for large matrices
- log det(A) = sum of log eigenvalues

---

## 10. Eigenvalues and Eigenvectors

### The Most Important Concept in ML

**Definition:**

For square matrix A, eigenvalue Œª and eigenvector v satisfy:

```
Av = Œªv
```

**Intuition:**
- v is a special direction
- A just scales v by Œª
- Doesn't change direction!

### Finding Eigenvalues

**Characteristic equation:**
```
det(A - ŒªI) = 0
```

**Example:**
```
A = ‚é° 4  2 ‚é§
    ‚é¢ 1  3 ‚é•

det(‚é° 4-Œª   2  ‚é§) = 0
    ‚é¢  1   3-Œª ‚é•

(4-Œª)(3-Œª) - 2 = 0
Œª¬≤ - 7Œª + 10 = 0
(Œª - 5)(Œª - 2) = 0

Œª‚ÇÅ = 5, Œª‚ÇÇ = 2
```

### Finding Eigenvectors

**For each Œª, solve:**
```
(A - ŒªI)v = 0
```

**Properties:**
- n√ón matrix has n eigenvalues (counting multiplicities)
- May be complex even if A is real
- Eigenvectors are unique up to scaling

### Special Cases

**Symmetric Matrix (A = A·µÄ):**
- All eigenvalues are real
- Eigenvectors are orthogonal
- Can be diagonalized: A = QŒõQ·µÄ
- Q is orthogonal matrix (Q·µÄQ = I)

**Positive Definite Matrix:**
- All eigenvalues > 0
- Important for optimization
- Hessian at minimum is positive definite

**Diagonal Matrix:**
- Eigenvalues = diagonal elements
- Standard basis vectors are eigenvectors

### Spectral Theorem

**Any symmetric matrix A can be written as:**
```
A = QŒõQ·µÄ

Where:
- Œõ = diagonal matrix of eigenvalues
- Q = orthogonal matrix of eigenvectors
- Q·µÄQ = I
```

This is **eigenvalue decomposition** or **spectral decomposition**.

### Powers of Matrices

**If A = QŒõQ·µÄ, then:**
```
A¬≤ = (QŒõQ·µÄ)(QŒõQ·µÄ) = QŒõ¬≤Q·µÄ
A¬≥ = QŒõ¬≥Q·µÄ
A‚Åø = QŒõ‚ÅøQ·µÄ
```

**Computing A‚Åø:**
1. Eigendecompose once: O(n¬≥)
2. Raise diagonal Œõ to power n: O(n)
3. Reconstruct: O(n¬≤)

Much better than n matrix multiplications!

### In Machine Learning

**Principal Component Analysis (PCA):**
```
1. Compute covariance: Œ£ = X·µÄX/n
2. Eigendecompose: Œ£ = QŒõQ·µÄ
3. Principal components = eigenvectors
4. Variance explained = eigenvalues
```

**PageRank (Google's algorithm):**
- Largest eigenvector of link matrix
- Stationary distribution of random walk

**Stability Analysis:**
- Eigenvalues of Jacobian determine stability
- |Œª| > 1: unstable (exploding gradients)
- |Œª| < 1: stable (vanishing gradients)

**Spectral Clustering:**
- Eigenvalues of graph Laplacian
- Community detection

**Variance in data:**
- Eigenvalues show variance in each direction
- Large eigenvalue = high variance direction

---

## 11. Matrix Decompositions

### Why Decompose Matrices?

**Benefits:**
1. **Computational efficiency**: Faster operations
2. **Numerical stability**: More accurate
3. **Insight**: Understand structure
4. **Compression**: Store less data

### LU Decomposition

**A = LU**

Where:
- L = lower triangular
- U = upper triangular

**Used for:**
- Solving linear systems
- Computing determinants
- Matrix inversion

**Complexity:** O(n¬≥)

### QR Decomposition

**A = QR**

Where:
- Q = orthogonal (Q·µÄQ = I)
- R = upper triangular

**Used for:**
- Least squares problems
- Finding eigenvalues (QR algorithm)
- Orthogonalizing vectors

**Properties:**
- More stable than normal equations
- Gram-Schmidt is one method

### Cholesky Decomposition

**A = LL·µÄ**

Where:
- A must be positive definite
- L is lower triangular

**Used for:**
- Solving positive definite systems
- Sampling from multivariate Gaussian
- Faster than general LU (about 2x)

**Complexity:** O(n¬≥/3)

### Singular Value Decomposition (SVD)

**The most important decomposition in ML!**

**A = UŒ£V·µÄ**

Where:
- U: m√óm orthogonal (left singular vectors)
- Œ£: m√ón diagonal (singular values œÉ·µ¢ ‚â• 0)
- V: n√ón orthogonal (right singular vectors)

**Properties:**
- Works for ANY matrix (rectangular, singular, etc.)
- Singular values are always real and non-negative
- œÉ·µ¢ = ‚àöŒª·µ¢ where Œª·µ¢ are eigenvalues of A·µÄA

**Relationship to Eigen:**
- A·µÄA = VŒ£¬≤V·µÄ (V are eigenvectors of A·µÄA)
- AA·µÄ = UŒ£¬≤U·µÄ (U are eigenvectors of AA·µÄ)

**Geometric Interpretation:**
1. Rotate by V·µÄ
2. Scale by Œ£
3. Rotate by U

**Truncated SVD (Low-rank approximation):**
```
A ‚âà Œ£·µ¢‚Çå‚ÇÅ·µè œÉ·µ¢u·µ¢v·µ¢·µÄ  (keep largest k singular values)
```

**This is the best rank-k approximation of A!**

### SVD Applications in ML

**Dimensionality Reduction:**
```python
# Keep top k singular values
U, s, Vt = np.linalg.svd(X)
X_reduced = U[:, :k] @ np.diag(s[:k])
```

**Recommender Systems:**
- Matrix completion
- Collaborative filtering
- Netflix prize!

**Image Compression:**
- Store only top k singular values/vectors
- Huge compression with little quality loss

**PCA:**
- SVD of centered data = PCA
- Faster and more stable than covariance eigendecomp

**Pseudoinverse:**
```
A‚Å∫ = VŒ£‚Å∫U·µÄ
```
Where Œ£‚Å∫ inverts non-zero singular values.

**Numerical Rank:**
```
rank(A) = number of singular values > Œµ
```

**Condition Number:**
```
cond(A) = œÉ_max / œÉ_min
```
- Large condition number = ill-conditioned
- Small perturbations cause large changes

---

## 12. Norms and Distances

### Vector Norms

**Lp Norm:**
```
||x||_p = (|x‚ÇÅ|·µñ + |x‚ÇÇ|·µñ + ... + |x‚Çô|·µñ)^(1/p)
```

**Common cases:**

**L1 (Manhattan):**
```
||x||‚ÇÅ = |x‚ÇÅ| + |x‚ÇÇ| + ... + |x‚Çô|
```
- Sum of absolute values
- Encourages sparsity (Lasso regression)

**L2 (Euclidean):**
```
||x||‚ÇÇ = ‚àö(x‚ÇÅ¬≤ + x‚ÇÇ¬≤ + ... + x‚Çô¬≤)
```
- Standard distance
- Used in Ridge regression
- Smooth, differentiable

**L‚àû (Maximum):**
```
||x||‚àû = max(|x‚ÇÅ|, |x‚ÇÇ|, ..., |x‚Çô|)
```
- Largest component
- Used in minimax problems

**L0 "Norm" (not actually a norm):**
```
||x||‚ÇÄ = number of non-zero elements
```
- Sparsity measure
- NP-hard to optimize
- Approximated by L1

### Properties of Norms

1. **Non-negativity:** ||x|| ‚â• 0
2. **Zero:** ||x|| = 0 ‚ü∫ x = 0
3. **Scaling:** ||Œ±x|| = |Œ±| √ó ||x||
4. **Triangle inequality:** ||x + y|| ‚â§ ||x|| + ||y||

### Matrix Norms

**Frobenius Norm:**
```
||A||_F = ‚àö(Œ£·µ¢‚±º |a·µ¢‚±º|¬≤)
```
- Like L2 for matrices
- Sum of squared elements

**Operator Norms:**
```
||A||_p = max_{||x||_p=1} ||Ax||_p
```

**Spectral Norm (2-norm):**
```
||A||‚ÇÇ = œÉ_max(A)  (largest singular value)
```

**Nuclear Norm:**
```
||A||_* = Œ£·µ¢ œÉ·µ¢  (sum of singular values)
```
- Convex relaxation of rank
- Used in matrix completion

### Distances

**Euclidean Distance:**
```
d(x, y) = ||x - y||‚ÇÇ
```

**Manhattan Distance:**
```
d(x, y) = ||x - y||‚ÇÅ
```

**Cosine Similarity (not a distance):**
```
sim(x, y) = (x ¬∑ y) / (||x|| √ó ||y||)
```
- Measures angle between vectors
- Range: [-1, 1]
- 1 = same direction, -1 = opposite

**Cosine Distance:**
```
d(x, y) = 1 - sim(x, y)
```

**Mahalanobis Distance:**
```
d(x, y) = ‚àö((x-y)·µÄŒ£‚Åª¬π(x-y))
```
- Accounts for covariance
- Scale-invariant

### In Machine Learning

**Regularization:**
```
L1: minimize loss + Œª||w||‚ÇÅ  (Lasso)
L2: minimize loss + Œª||w||‚ÇÇ¬≤  (Ridge)
```

**Distance metrics:**
- K-NN: Euclidean or Manhattan
- Word embeddings: Cosine similarity
- Anomaly detection: Mahalanobis

**Optimization:**
- Gradient norm for convergence: ||‚àáf|| < Œµ

**Robustness:**
- L‚àû norm for adversarial robustness

---

## 13. Orthogonality and Projections

### Orthogonality

**Vectors are orthogonal if:**
```
u ¬∑ v = 0
```

**Geometric meaning:**
- Perpendicular (90¬∞ angle)
- No component in each other's direction

**Orthonormal:**
- Orthogonal AND unit length
- ||u|| = ||v|| = 1
- u ¬∑ v = 0

**Orthogonal Matrix Q:**
```
Q·µÄQ = QQ·µÄ = I
```

**Properties:**
- Columns are orthonormal
- Rows are orthonormal
- Preserves lengths: ||Qx|| = ||x||
- Preserves angles: (Qx) ¬∑ (Qy) = x ¬∑ y
- det(Q) = ¬±1
- Q‚Åª¬π = Q·µÄ (cheap to invert!)

### Projections

**Project vector b onto vector a:**
```
proj_a(b) = (a ¬∑ b / a ¬∑ a) √ó a
```

**Explicitly:**
```
proj_a(b) = (a·µÄb / a·µÄa) √ó a
```

**If a is unit vector:**
```
proj_a(b) = (a·µÄb) √ó a
```

**Geometric meaning:**
- Shadow of b on a
- Component of b in direction of a

**Project onto subspace spanned by columns of A:**
```
proj_A(b) = A(A·µÄA)‚Åª¬πA·µÄb
```

**Projection matrix:**
```
P = A(A·µÄA)‚Åª¬πA·µÄ
```

**Properties:**
- P¬≤ = P (idempotent)
- P·µÄ = P (symmetric)
- Pb gives projection of b onto column space of A

### Gram-Schmidt Process

**Orthogonalize vectors {v‚ÇÅ, v‚ÇÇ, ..., v‚Çô}:**

```
u‚ÇÅ = v‚ÇÅ
u‚ÇÇ = v‚ÇÇ - proj_{u‚ÇÅ}(v‚ÇÇ)
u‚ÇÉ = v‚ÇÉ - proj_{u‚ÇÅ}(v‚ÇÉ) - proj_{u‚ÇÇ}(v‚ÇÉ)
...
```

**Then normalize:**
```
e·µ¢ = u·µ¢ / ||u·µ¢||
```

**Result:** Orthonormal basis {e‚ÇÅ, e‚ÇÇ, ..., e‚Çô}

### In Machine Learning

**QR Decomposition:**
- Gram-Schmidt gives Q and R

**PCA:**
- Principal components are orthogonal

**Whitening:**
- Transform data to have orthogonal features

**Least Squares:**
```
Ax = b has no exact solution
Solution: xÃÇ = (A·µÄA)‚Åª¬πA·µÄb
Geometric: project b onto column space of A
```

**Residuals:**
```
residual = b - AxÃÇ  (orthogonal to column space)
```

---

## 14. Computational Considerations

### Why Care About Computation?

**Modern ML deals with:**
- Millions of parameters
- Billions of data points
- Need for real-time inference

**Bad algorithms ‚Üí Training for years**
**Good algorithms ‚Üí Training in hours**

### Matrix Multiplication Efficiency

**Naive algorithm: O(n¬≥)**
```python
# Don't do this!
for i in range(n):
    for j in range(n):
        for k in range(n):
            C[i,j] += A[i,k] * B[k,j]
```

**Use optimized libraries:**
```python
# Do this!
C = A @ B  # NumPy uses BLAS
C = torch.mm(A, B)  # PyTorch uses cuBLAS on GPU
```

**Speedup: 100-1000x with good BLAS!**

### Memory Layout

**Row-major (C, NumPy default):**
```
[1, 2, 3, 4, 5, 6]  # for 2√ó3 matrix
```

**Column-major (Fortran, MATLAB):**
```
[1, 4, 2, 5, 3, 6]  # for 2√ó3 matrix
```

**Why care?**
- Cache efficiency
- Accessing rows vs columns speed

**Best practice:**
- Access in storage order
- NumPy: iterate rows, not columns

### Avoiding Matrix Inversion

**Never compute A‚Åª¬π explicitly!**

**Bad:**
```python
x = np.linalg.inv(A) @ b  # O(n¬≥) + numerical issues
```

**Good:**
```python
x = np.linalg.solve(A, b)  # More stable, same complexity
```

**Why?**
- inv(A) may not exist due to numerical errors
- Solving is more stable
- Don't need full inverse

### Sparse Matrices

**If most elements are zero:**
```python
from scipy.sparse import csr_matrix

# Dense: store n¬≤  values
A_dense = np.array([[1, 0, 0], [0, 2, 0], [0, 0, 3]])

# Sparse: store only non-zeros
A_sparse = csr_matrix(A_dense)
```

**Benefits:**
- Memory: O(nnz) instead of O(n¬≤)
- Speed: Operations on non-zeros only

**Used in:**
- NLP (TF-IDF matrices)
- Graph algorithms
- Finite element methods

### GPU Acceleration

**GPUs excel at:**
- Matrix multiplication
- Element-wise operations
- Parallel operations

**CPU vs GPU:**
```
Matrix multiply (1000√ó1000):
CPU: ~100 ms
GPU: ~5 ms
Speedup: 20x
```

**When to use GPU:**
- Large matrices (>1000√ó1000)
- Batch operations
- Deep learning training

**PyTorch example:**
```python
A = torch.randn(1000, 1000).cuda()
B = torch.randn(1000, 1000).cuda()
C = A @ B  # Runs on GPU
```

### Numerical Stability

**Problems:**

1. **Catastrophic cancellation:**
```python
# Bad: subtracting nearly equal numbers
x = 1.0000001
y = 1.0000000
diff = x - y  # Lost precision!
```

2. **Overflow/Underflow:**
```python
# Exponentials can overflow
exp(1000)  # inf!

# Use log-sum-exp trick:
log(exp(a) + exp(b)) = max(a,b) + log(1 + exp(-|a-b|))
```

3. **Ill-conditioned matrices:**
```python
# Large condition number ‚Üí unstable
cond = np.linalg.cond(A)
if cond > 1e10:
    # Add regularization!
    A_reg = A + Œª*I
```

### Best Practices

1. **Use library functions**
   - NumPy, SciPy, PyTorch
   - Optimized C/Fortran implementations

2. **Vectorize operations**
   - Avoid Python loops
   - Use broadcast operations

3. **Choose right data types**
   - float32 vs float64
   - Sparse vs dense

4. **Profile your code**
   - Find bottlenecks
   - Optimize what matters

5. **Numerical stability**
   - Avoid inverting matrices
   - Use stable algorithms (SVD > eigendecomp)
   - Add regularization when needed

---

## 15. ML Applications

### Linear Regression

**Model:**
```
y = Xw + b
```

**Matrix form:**
```
      ‚é°   ‚Äî‚Äî x‚ÇÅ·µÄ ‚Äî‚Äî   ‚é§       ‚é° y‚ÇÅ ‚é§
X =   ‚é¢   ‚Äî‚Äî x‚ÇÇ·µÄ ‚Äî‚Äî   ‚é•   y = ‚é¢ y‚ÇÇ ‚é•
      ‚é¢       ‚ãÆ         ‚é•       ‚é¢  ‚ãÆ ‚é•
      ‚é£   ‚Äî‚Äî x‚Çô·µÄ ‚Äî‚Äî   ‚é¶       ‚é£ y‚Çô ‚é¶
```

**Solution (closed form):**
```
w = (X·µÄX)‚Åª¬πX·µÄy
```

**Linear algebra:**
- X·µÄX: covariance-like matrix
- (X·µÄX)‚Åª¬πX·µÄ: pseudoinverse
- Projection onto column space

### Principal Component Analysis (PCA)

**Goal:** Find directions of maximum variance

**Algorithm:**
```
1. Center data: XÃÉ = X - mean(X)
2. Compute covariance: Œ£ = XÃÉ·µÄXÃÉ / n
3. Eigendecompose: Œ£ = QŒõQ·µÄ
4. Principal components = columns of Q
5. Variance explained = diagonal of Œõ
```

**Dimensionality reduction:**
```
X_reduced = XÃÉ @ Q[:, :k]  # Keep top k components
```

**Using SVD (better):**
```
XÃÉ = UŒ£V·µÄ
PC = V  (right singular vectors)
```

### Neural Networks

**Single layer:**
```
z = Wx + b  # Linear transformation
a = œÉ(z)    # Nonlinear activation
```

**Multi-layer:**
```
a‚ÇÄ = x
a‚ÇÅ = œÉ(W‚ÇÅa‚ÇÄ + b‚ÇÅ)
a‚ÇÇ = œÉ(W‚ÇÇa‚ÇÅ + b‚ÇÇ)
...
```

**Backpropagation = Chain rule + Matrix calculus**

**Gradient of loss w.r.t. weights:**
```
‚àÇL/‚àÇW = ‚àÇL/‚àÇz √ó ‚àÇz/‚àÇW
```

All matrix operations!

### Transformers (Attention)

**Self-Attention:**
```
Q = XW_Q  (queries)
K = XW_K  (keys)
V = XW_V  (values)

Attention = softmax(QK·µÄ/‚àöd) V
```

**Pure matrix operations!**
- QK·µÄ: all pairwise dot products
- Softmax row-wise
- Multiply by values

**Multi-head attention:**
- Multiple W_Q, W_K, W_V
- Parallel matrix multiplications
- Concat and project

### Recommender Systems

**Matrix Factorization:**
```
R ‚âà UV ·µÄ
```
- R: user-item ratings (sparse!)
- U: user embeddings
- V: item embeddings

**SVD for collaborative filtering:**
```
R = UŒ£V·µÄ
Keep top k singular values
Predict missing entries
```

### Eigenfaces

**Face recognition using PCA:**
```
1. Collect face images as columns of X
2. Compute mean face: Œº = mean(X, axis=1)
3. Center: XÃÉ = X - Œº
4. SVD: XÃÉ = UŒ£V·µÄ
5. Eigenfaces = columns of U
6. Project faces: coeffs = U·µÄ(face - Œº)
7. Compare coefficients for recognition
```

### Graph Algorithms

**PageRank:**
```
score = (A)score
```
- Largest eigenvector of link matrix
- Power iteration to find it

**Spectral Clustering:**
```
1. Build similarity matrix S
2. Compute Laplacian: L = D - S
3. Find k smallest eigenvectors
4. K-means on eigenvectors
```

---

## Summary

### Key Takeaways

1. **Linear algebra is the language of ML**
   - Every algorithm uses it
   - Understanding it deeply is essential

2. **Geometric intuition matters**
   - Matrices are transformations
   - Eigenvalues show important directions
   - SVD reveals structure

3. **Computational efficiency is critical**
   - O(n¬≥) doesn't scale
   - Use optimized libraries
   - GPU acceleration

4. **Numerical stability is important**
   - Avoid matrix inversion
   - Use stable algorithms
   - Regularize when needed

5. **Applications everywhere**
   - Regression, PCA, neural nets
   - Transformers, recommenders
   - Graph algorithms

### Master These

**Essential Operations:**
- Matrix multiplication
- Transpose
- Dot product
- Norms

**Essential Decompositions:**
- Eigenvalue decomposition
- SVD
- QR decomposition

**Essential Concepts:**
- Linear transformations
- Orthogonality
- Projections
- Rank

### What's Next?

**Practice:**
- Implement algorithms from scratch
- Solve the problems in problems.md
- Visualize transformations

**Apply:**
- Build PCA from scratch
- Implement linear regression
- Understand neural network math

**Deep Dive:**
- Matrix calculus (for backprop)
- Optimization theory
- Numerical linear algebra

---

**You now have the foundation. Time to practice!** üöÄ

**Next**: Work through `examples.py` to see these concepts in code!
