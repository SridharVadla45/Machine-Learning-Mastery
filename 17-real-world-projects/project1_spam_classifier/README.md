# Project 1: Email Spam Classifier ğŸ“§

## ğŸ¯ Project Overview

Build a production-ready spam email classifier using machine learning and NLP techniques. This is a **beginner-friendly** project that covers the complete ML workflow from data to deployment.

**Business Problem**: Automatically identify spam emails to improve user experience and reduce security risks.

**Solution**: Binary classification system using TF-IDF and machine learning.

---

## ğŸ“Š Project Specifications

- **Type**: Binary Classification
- **Domain**: Natural Language Processing
- **Dataset**: 5,572 emails (spam and ham)
- **Target Metric**: 95%+ accuracy
- **Deployment**: FastAPI REST API
- **Time to Complete**: 8-10 hours

---

## ğŸ“ Learning Objectives

By completing this project, you will:

1. âœ… Understand text preprocessing pipeline
2. âœ… Implement TF-IDF vectorization
3. âœ… Train and compare multiple classifiers
4. âœ… Evaluate models with appropriate metrics
5. âœ… Build a REST API for predictions
6. âœ… Deploy ML model in production
7. âœ… Write tests for ML systems

---

## ğŸ“ Project Structure

```
project1_spam_classifier/
â”œâ”€â”€ README.md                  # This file
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ spam.csv          # Original dataset
â”‚   â”œâ”€â”€ processed/
â”‚   â”‚   â”œâ”€â”€ train.csv         # Training data
â”‚   â”‚   â””â”€â”€ test.csv          # Test data
â”‚   â””â”€â”€ download.py           # Data download script
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb          # Exploratory analysis
â”‚   â”œâ”€â”€ 02_modeling.ipynb     # Model development
â”‚   â””â”€â”€ 03_evaluation.ipynb   # Results and interpretation
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data.py               # Data loading and preprocessing
â”‚   â”œâ”€â”€ features.py           # Feature engineering (TF-IDF)
â”‚   â”œâ”€â”€ models.py             # Model definitions
â”‚   â”œâ”€â”€ train.py              # Training script
â”‚   â”œâ”€â”€ evaluate.py           # Evaluation metrics
â”‚   â””â”€â”€ predict.py            # Inference
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ app.py                # FastAPI application
â”‚   â”œâ”€â”€ schemas.py            # Request/response schemas
â”‚   â””â”€â”€ requirements.txt      # API dependencies
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_data.py
â”‚   â”œâ”€â”€ test_features.py
â”‚   â”œâ”€â”€ test_models.py
â”‚   â””â”€â”€ test_api.py
â”œâ”€â”€ deployment/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ docker-compose.yml
â”œâ”€â”€ models/
â”‚   â””â”€â”€ trained/
â”‚       â”œâ”€â”€ model.pkl         # Trained model
â”‚       â””â”€â”€ vectorizer.pkl    # TF-IDF vectorizer
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ metrics.json          # Performance metrics
â”‚   â””â”€â”€ plots/
â”‚       â”œâ”€â”€ confusion_matrix.png
â”‚       â”œâ”€â”€ roc_curve.png
â”‚       â””â”€â”€ word_clouds.png
â””â”€â”€ requirements.txt          # Project dependencies
```

---

## ğŸš€ Quick Start

### 1. Setup Environment

```bash
# Navigate to project directory
cd 17-real-world-projects/project1_spam_classifier

# Install dependencies
uv pip install -r requirements.txt

# Download data
uv run data/download.py
```

### 2. Explore Data

```bash
# Open Jupyter
jupyter notebook notebooks/01_eda.ipynb
```

### 3. Train Model

```bash
# Run training script
uv run src/train.py

# Expected output:
# Training model...
# Accuracy: 96.5%
# Model saved to models/trained/
```

### 4. Evaluate

```bash
# Run evaluation
uv run src/evaluate.py

# Generates:
# - Confusion matrix
# - ROC curve
# - Classification report
```

### 5. Start API

```bash
# Run API server
cd api
uv run app.py

# API available at: http://localhost:8000
# Docs at: http://localhost:8000/docs
```

### 6. Test Predictions

```bash
# Test API
curl -X POST "http://localhost:8000/predict" \
  -H "Content-Type: application/json" \
  -d '{"text": "Congratulations! You won $1000!"}'

# Response: {"prediction": "spam", "confidence": 0.95}
```

---

## ğŸ“š Detailed Walkthrough

### Phase 1: Data Understanding (Day 1)

**Objectives**:
- Understand dataset structure
- Analyze class distribution
- Identify data quality issues
- Explore text characteristics

**Notebook**: `01_eda.ipynb`

**Key Insights**:
- Dataset is slightly imbalanced (13% spam)
- Spam emails have more CAPS, numbers, special chars
- Average spam length: 138 words vs ham: 71 words
- Common spam words: free, win, click, money

### Phase 2: Data Preprocessing (Day 2)

**Steps**:
1. Text cleaning (lowercase, remove special chars)
2. Tokenization
3. Stop word removal
4. Lemmatization
5. Train/test split (80/20)

**Code**: `src/data.py`

**Example**:
```python
from src.data import preprocess_text

text = "FREE MONEY!!! Click here NOW!!!"
cleaned = preprocess_text(text)
# Output: "free money click"
```

### Phase 3: Feature Engineering (Day 3)

**Approach**: TF-IDF (Term Frequency-Inverse Document Frequency)

**Why TF-IDF?**
- Captures word importance
- Reduces impact of common words
- Better than simple word counts
- Works well for spam detection

**Code**: `src/features.py`

**Parameters**:
- max_features: 3000
- ngram_range: (1, 2) # unigrams and bigrams
- min_df: 5 # minimum document frequency

### Phase 4: Model Training (Day 4-5)

**Models Compared**:
1. Naive Bayes (baseline)
2. Logistic Regression
3. Random Forest
4. Support Vector Machine (SVM)

**Best Model**: Logistic Regression
- Accuracy: 96.5%
- Precision: 97.2%
- Recall: 94.8%
- F1-Score: 96.0%

**Code**: `src/train.py`

### Phase 5: Evaluation (Day 6)

**Metrics**:
- Confusion Matrix
- ROC Curve (AUC: 0.98)
- Precision-Recall Curve
- Classification Report

**Code**: `src/evaluate.py`

**Key Results**:
- Very few false positives (ham marked as spam)
- Acceptable false negatives (spam marked as ham)
- Robust to different email styles

### Phase 6: API Development (Day 7)

**API Endpoints**:

```python
POST /predict
{
  "text": "email content here"
}

Response:
{
  "prediction": "spam" | "ham",
  "confidence": 0.95,
  "processing_time_ms": 12
}
```

**Code**: `api/app.py`

**Features**:
- Input validation
- Error handling
- Logging
- Metrics tracking

### Phase 7: Testing (Day 8)

**Test Coverage**:
- Unit tests for preprocessing
- Integration tests for pipeline
- API endpoint tests
- Performance tests

**Code**: `tests/`

**Run Tests**:
```bash
pytest tests/ -v --cov=src
```

### Phase 8: Deployment (Day 9-10)

**Docker Setup**:
```bash
# Build image
docker build -t spam-classifier .

# Run container
docker run -p 8000:8000 spam-classifier
```

**Production Considerations**:
- Model versioning
- Monitoring (latency, accuracy drift)
- A/B testing framework
- Logging and alerting

---

## ğŸ“Š Results

### Model Performance

| Metric | Value |
|--------|-------|
| Accuracy | 96.5% |
| Precision | 97.2% |
| Recall | 94.8% |
| F1-Score | 96.0% |
| AUC-ROC | 0.98 |

### Inference Speed

- Average: 12ms per email
- 99th percentile: 25ms
- Throughput: ~80 requests/second

### Resource Usage

- Model size: 2.4 MB
- Memory: ~150 MB
- CPU: Single core sufficient

---

## ğŸ¯ Success Criteria

âœ… **Achieved**:
- [x] Accuracy > 95%
- [x] API response time < 100ms
- [x] Complete test coverage
- [x] Documented code
- [x] Deployable Docker container

---

## ğŸš§ Challenges & Solutions

### Challenge 1: Imbalanced Dataset
**Problem**: Only 13% spam emails  
**Solution**: Used stratified split, monitored precision/recall

### Challenge 2: Overfitting
**Problem**: Perfect training accuracy  
**Solution**: Added regularization, used cross-validation

### Challenge 3: Model Size
**Problem**: Large vocabulary â†’ large model  
**Solution**: Limited max_features to 3000, kept model < 5MB

---

## ğŸ”„ Next Steps & Improvements

### Immediate Improvements:
1. Add more features (email metadata, sender info)
2. Try deep learning (LSTM, BERT)
3. Implement online learning
4. Add explainability (LIME/SHAP)

### Production Enhancements:
1. Set up MLflow for experiment tracking
2. Implement model monitoring
3. Add A/B testing framework
4. Create feedback loop for model updates

### Advanced Features:
1. Multi-class classification (spam types)
2. Confidence calibration
3. Active learning for edge cases
4. Multi-language support

---

## ğŸ“– Key Takeaways

1. **Text Preprocessing Matters**: Cleaning improves accuracy by 5-10%
2. **Simple Models Work**: Logistic Regression beats complex models here
3. **Feature Engineering > Model Complexity**: TF-IDF crucial for performance
4. **Production != Development**: Need API, tests, monitoring
5. **Iterate Quickly**: Start simple, improve incrementally

---

## ğŸ“ Skills Demonstrated

- âœ… Text preprocessing and cleaning
- âœ… TF-IDF feature extraction
- âœ… Binary classification
- âœ… Model evaluation and selection
- âœ… REST API development (FastAPI)
- âœ… Unit and integration testing
- âœ… Docker containerization
- âœ… ML pipeline orchestration

---

## ğŸ“š Additional Resources

### Documentation:
- [scikit-learn TF-IDF](https://scikit-learn.org/stable/modules/feature_extraction.html#tfidf-term-weighting)
- [FastAPI Docs](https://fastapi.tiangolo.com/)
- [Docker Best Practices](https://docs.docker.com/develop/dev-best-practices/)

### Related Reading:
- "Spam Filtering with Naive Bayes" (Paul Graham)
- "Text Classification" (scikit-learn documentation)
- "Building ML APIs" (FastAPI tutorials)

---

## ğŸ¤ Contributing

Found a bug? Have improvement ideas?
- Open an issue
- Submit a pull request
- Share your results

---

## âœ¨ Congratulations!

You've built a production-ready spam classifier! ğŸ‰

**What you accomplished**:
- Complete ML pipeline from data to deployment
- Production-quality code with tests
- Deployable system with API
- Strong portfolio project

**Next Project**: `project2_house_prices/` - Regression and feature engineering

---

**Questions?** Review the notebooks and source code.  
**Stuck?** Check `solutions/` directory for complete implementation.  
**Finished?** Share your project and deploy it live!

**Happy Building! ğŸš€**
